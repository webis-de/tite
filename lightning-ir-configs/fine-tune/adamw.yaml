optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 2e-5
# lr_scheduler:
#   class_path: tite.utils.lr_schedulers.SigmoidLRSchedulerWithLinearWarmup
#   init_args:
#     num_warmup_steps: 3000
#     final_value: 0.02
lr_scheduler:
  class_path: tite.utils.lr_schedulers.ConstantLRSchedulerWithLinearWarmup
  init_args:
    num_warmup_steps: 3000
