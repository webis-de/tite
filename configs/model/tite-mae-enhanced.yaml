model:
  class_path: tite.module.TiteModule
  init_args:
    student:
      class_path: tite.model.TiteModel
      init_args:
        config:
          class_path: tite.model.TiteConfig
          init_args:
            vocab_size: 30522
            num_hidden_layers: 12
            # hidden_sizes: [768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768]
            # num_attention_heads: [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]
            # intermediate_sizes: [3072, 3072, 3072, 3072, 3072, 3072, 3072, 3072, 3072, 3072, 3072, 3072]
            # hidden_sizes: [768, 768, 768, 1024, 1024, 1024, 1280, 1280, 1280, 1536, 1536, 1536]
            # num_attention_heads: [12, 12, 12, 16, 16, 16, 20, 20, 20, 24, 24, 24]
            # intermediate_sizes: [3072, 3072, 3072, 4096, 4096, 4096, 5120, 5120, 5120, 6144, 6144, 6144]
            # hidden_sizes: [768, 768, 768, 1024, 1280, 1536, 1792, 2048, 2304, 2560, 2816, 3072]
            # num_attention_heads: [12, 12, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48]
            # intermediate_sizes: [3072, 3072, 3072, 4096, 5120, 6144, 7168, 8192, 9216, 10240, 11264, 12288]
            # hidden_sizes: [768, 1024, 1024, 1280, 1280, 1536, 1536, 1792, 1792, 2048, 2048, 2304]
            # num_attention_heads: [12, 16, 16, 20, 20, 24, 24, 28, 28, 32, 32, 36]
            # intermediate_sizes: [3072, 4096, 4096, 5120, 5120, 6144, 6144, 7168, 7168, 8192, 8192, 9216]
            # kernel_sizes: [null, null, null, null, null, null, 3, 3, 3, 3, 3, 3]
            # strides: [null, null, null, null, null, null, 3, 3, 3, 3, 3, 3]
            # kernel_sizes: [null, 3, null, 3, null, 3, null, 3, null, 3, null, 3]
            # strides: [null, 3, null, 3, null, 3, null, 3, null, 3, null, 3]
            kernel_sizes: [null, null, null, 2, 2, 2, 2, 2, 2, 2, 2, 2]
            strides: [null, null, null, 2, 2, 2, 2, 2, 2, 2, 2, 2]
            # kernel_sizes: [null, 2, 2, 2, null, 2, 2, 2, null, 2, 2, 2]
            # strides: [null, 2, 2, 2, null, 2, 2, 2, null, 2, 2, 2]
            # hidden_sizes: [768, 1024, 1280, 1536, 1536, 1792, 2048, 2304, 2304, 2560, 2816, 3072]
            # num_attention_heads: [12, 16, 20, 24, 24, 28, 32, 36, 36, 40, 44, 48]
            # intermediate_sizes: [3072, 4096, 5120, 6144, 6144, 7168, 8192, 9216, 9216, 10240, 11264, 12288]
            # hidden_sizes: [768, 1024, 1024, 1024, 1024, 1280, 1280, 1280, 1280, 1536, 1536, 1536]
            # num_attention_heads: [12, 16, 16, 16, 16, 20, 20, 20, 20, 24, 24, 24]
            # intermediate_sizes: [3072, 4096, 4096, 4096, 4096, 5120, 5120, 5120, 5120, 6144, 6144, 6144]
            hidden_sizes: [1536, 1536, 1536, 1536, 1536, 1536, 1536, 1536, 1536, 1536, 1536, 1536]
            num_attention_heads: [24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24]
            intermediate_sizes: [6144, 6144, 6144, 6144, 6144, 6144, 6144, 6144, 6144, 6144, 6144, 6144]
            positional_embedding_type: rotary
            upscale_hidden_sizes: true
            attention_based_pooling: true
            pooling_strategy: mean_conv
    tokenizer:
      class_path: tite.tokenizer.TiteTokenizer
      init_args:
        vocab_file: tokenizers/tite/vocab.txt
        tokenizer_file: tokenizers/tite/tokenizer.json
        do_lower_case: True
        unk_token: '[UNK]'
        sep_token: '[SEP]'
        pad_token: '[PAD]'
        cls_token: '[CLS]'
        mask_token: '[MASK]'
      dict_kwargs:
        model_max_length: 512
    teachers:
    - class_path: tite.teacher.MAETeacher
      init_args:
        padid: 0
        enhanced: true
    predictors:
    - class_path: tite.predictor.MAEEnhancedDecoder
      init_args:
        hidden_size: 768
        num_attention_heads: 12
        intermediate_size: 3072
        # hidden_size: 1536
        # num_attention_heads: 24
        # intermediate_size: 6144
        # hidden_size: 3072
        # num_attention_heads: 48
        # intermediate_size: 12288
        query_strategy: embx
        mask_id: 103
        mask_prob: 0.5
    losses:
    - class_path: tite.loss.MAECrossEntropy
      init_args:
        vocab_size: 30522
    log_additional_metrics: false
    validate_on_glue: true
    validate_on_msmarco: true
    log_gradients: true