model:
  class_path: tite.module.TiteModule
  init_args:
    model:
      class_path: tite.model.TiteForPreTraining
      init_args:
        config:
          class_path: tite.model.TiteConfig
          init_args:
            vocab_size: 30522
            num_hidden_layers: 12
            kernel_sizes: [null, null, null, 2, 2, 2, 2, 2, 2, 2, 2, 2]
            strides: [null, null, null, 2, 2, 2, 2, 2, 2, 2, 2, 2]
            hidden_sizes: [768, 768, 768, 1024, 1024, 1024, 1280, 1280, 1280, 1536, 1536, 1536]
            num_attention_heads: [12, 12, 12, 16, 16, 16, 20, 20, 20, 24, 24, 24]
            intermediate_sizes: [3072, 3072, 3072, 4096, 4096, 4096, 5120, 5120, 5120, 6144, 6144, 6144]
            positional_embedding_type: rotary
            rotary_interleaved: true
            hidden_act: gelu_pytorch_tanh
            norm_location: post
            norm_type: layer
            qk_norm: true
        enhanced_masked_auto_encoding: true
        enhanced_causal_auto_encoding: false
        bow_auto_encoding: true
    tokenizer:
      class_path: tite.model.TiteTokenizer
      init_args:
        vocab_file: tokenizers/tite/vocab.txt
        tokenizer_file: tokenizers/tite/tokenizer.json
        do_lower_case: True
        unk_token: '[UNK]'
        sep_token: '[SEP]'
        pad_token: '[PAD]'
        cls_token: '[CLS]'
        mask_token: '[MASK]'
      dict_kwargs:
        model_max_length: 512
    validate_on_glue: true
    validate_on_trec_dl: true
    log_gradients: false
