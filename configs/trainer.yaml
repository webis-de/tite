# lightning.pytorch==2.2.0.post0
seed_everything: 42
trainer:
  accelerator: auto
  strategy: auto
  devices: auto
  num_nodes: 1
  precision: bf16-mixed
  logger:
    class_path: CustomWandbLogger
    init_args:
      project: tite
    dict_kwargs:
      entity: tite
  callbacks:
    - class_path: ModelCheckpoint
      init_args:
        save_on_train_epoch_end: false
        save_top_k: -1
    - class_path: LearningRateMonitor
      init_args:
        logging_interval: step
  max_steps: 70000
  val_check_interval: 64000
  accumulate_grad_batches: 64
  gradient_clip_val: 10
  detect_anomaly: false
  inference_mode: false
  enable_progress_bar: false