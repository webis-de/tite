optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 5e-4
# lr_scheduler:
#   class_path: LinearLRSchedulerWithLinearWarmup
#   init_args:
#     num_warmup_steps: 3000
#     final_value: 0.02
# lr_scheduler:
#   class_path: ConstantLRSchedulerWithLinearWarmup
#   init_args:
#     num_warmup_steps: 3000