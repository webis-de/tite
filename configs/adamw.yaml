optimizer:
  class_path: tite.utils.adamw.AdamWNoWeightDecayBiasNorm
  # class_path: torch.optim.AdamW
  init_args:
    lr: 1e-4
# lr_scheduler:
#   class_path: LinearLRSchedulerWithLinearWarmup
#   init_args:
#     num_warmup_steps: 3000
#     final_value: 0.02
lr_scheduler:
  class_path: SigmoidLRSchedulerWithLinearWarmup
  init_args:
    num_warmup_steps: 3000
    final_value: 0.02
# lr_scheduler:
#   class_path: ConstantLRSchedulerWithLinearWarmup
#   init_args:
#     num_warmup_steps: 3000